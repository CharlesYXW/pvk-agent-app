import streamlit as st
import pandas as pd
from dotenv import load_dotenv

# åœ¨æ‰€æœ‰ä»£ç æ‰§è¡Œå‰ï¼Œé¦–å…ˆåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from sklearn.ensemble import RandomForestRegressor
import itertools
import os
import arxiv
import datetime

# LangChainç›¸å…³çš„åº“ï¼ˆä»…ç”¨äºæ£€ç´¢ï¼‰
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings

# DashScopeå®˜æ–¹SDK
from dashscope import Generation

# --- æ ¸å¿ƒåŠŸèƒ½é€»è¾‘ ---

@st.cache_resource
def get_retriever():
    if not os.path.exists("faiss_index"): return None
    try:
        embeddings = SentenceTransformerEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")
        vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        return vectorstore.as_retriever(search_kwargs={"k": 3})
    except Exception as e:
        st.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        return None

# æ–°å¢ï¼šä½¿ç”¨DashScope SDKè°ƒç”¨æ¨¡å‹çš„è¾…åŠ©å‡½æ•°
def call_qwen_model(messages):
    try:
        response = Generation.call(
            model="qwen-flash",
            messages=messages,
            result_format="message",
        )
        if response.status_code == 200:
            return response.output.choices[0].message.content
        else:
            return f"è°ƒç”¨å¤§æ¨¡å‹æ—¶å‡ºé”™ï¼š{response.message}"
    except Exception as e:
        return f"è°ƒç”¨å¤§æ¨¡å‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}"

@st.cache_data
def get_latest_briefing(keywords):
    """è·å–æœ€æ–°æ–‡çŒ®ç®€æŠ¥ã€‚ä½¿ç”¨ç¼“å­˜é¿å…çŸ­æ—¶é—´å†…é‡å¤è¯·æ±‚ã€‚"""
    if not keywords or not any(keywords):
        return "è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…³é”®è¯ã€‚"

    MAX_RESULTS_PER_KEYWORD = 3
    unique_papers = {}
    for query in keywords:
        try:
            search = arxiv.Search(query=query, max_results=MAX_RESULTS_PER_KEYWORD, sort_by=arxiv.SortCriterion.LastUpdatedDate)
            for result in search.results():
                if result.entry_id not in unique_papers: unique_papers[result.entry_id] = result
        except Exception as e: return f"æ£€ç´¢æ—¶å‡ºé”™: {e}"
    if not unique_papers: return "æœªæ‰¾åˆ°ä¸æ‚¨å…³é”®è¯ç›¸å…³çš„æ–°è®ºæ–‡ã€‚"
    
    report_content = [f"# ç§‘ç ”ç®€æŠ¥ ({datetime.date.today().strftime('%Y-%m-%d')})\n\n"]
    report_content.append(f"**æ£€ç´¢å…³é”®è¯:** `{', '.join(keywords)}`\n\n---\n\n")
    for i, paper in enumerate(unique_papers.values()):
        summary = paper.summary.replace('\n', ' ')
        content = (
            f"## {i+1}. {paper.title}\n\n"
            f"- **ä½œè€…:** {', '.join(author.name for author in paper.authors)}\n"
            f"- **é“¾æ¥:** {paper.pdf_url}\n\n"
            f"**æ‘˜è¦:**\n> {summary}\n\n"
            f"---\n"
        )
        report_content.append(content)
    return "".join(report_content)

def analyze_xrd_from_upload(uploaded_file):
    # ... (æ­¤å‡½æ•°ä¸å˜)
    if uploaded_file is None: return None
    try:
        data = np.loadtxt(uploaded_file, comments="#", delimiter=",")
        angle, intensity = data[:, 0], data[:, 1]
    except Exception:
        st.error("æ–‡ä»¶è§£æå¤±è´¥ã€‚è¯·ç¡®ä¿æ˜¯ä¸¤åˆ—ï¼ˆè§’åº¦, å¼ºåº¦ï¼‰çš„CSVæˆ–TXTæ–‡ä»¶ã€‚")
        return None
    peaks, _ = find_peaks(intensity, height=np.mean(intensity), distance=10)
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(angle, intensity, label="XRD Spectrum"), ax.plot(angle[peaks], intensity[peaks], "x", markersize=8, label="Detected Peaks")
    for i in peaks: ax.annotate(f"{angle[i]:.2f}Â°", (angle[i], intensity[i]), textcoords="offset points", xytext=(0,5), ha='center')
    ax.set_title("XRD Spectrum Analysis"), ax.set_xlabel("2-Theta Angle (Â°)"), ax.set_ylabel("Intensity (A.U.)")
    ax.legend(), ax.grid(True, linestyle='--', alpha=0.6)
    return fig

@st.cache_resource
def get_trained_model():
    # ... (æ­¤å‡½æ•°ä¸å˜)
    if not os.path.exists("simulated_experimental_data.csv"): return None
    df = pd.read_csv("simulated_experimental_data.csv")
    features = ['spin_coating_rpm', 'annealing_temperature_C', 'additive_concentration_percent']
    target = 'efficiency_percent'
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(df[features], df[target])
    return model

def find_optimal_params(_model):
    # ... (æ­¤å‡½æ•°ä¸å˜)
    features = ['spin_coating_rpm', 'annealing_temperature_C', 'additive_concentration_percent']
    param_grid = list(itertools.product(np.arange(2500, 5501, 500), np.arange(80, 121, 10), np.arange(0.5, 1.51, 0.2)))
    grid_df = pd.DataFrame(param_grid, columns=features)
    predicted_efficiencies = _model.predict(grid_df)
    best_index = np.argmax(predicted_efficiencies)
    return grid_df.iloc[best_index], predicted_efficiencies[best_index]

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="é’™é’›çŸ¿ç ”å‘æ™ºèƒ½åŠ©æ‰‹", layout="wide")
st.title("ğŸ”¬ é’™é’›çŸ¿ç ”å‘æ™ºèƒ½åŠ©æ‰‹")

# --- å¯¼èˆª ---
with st.sidebar:
    st.markdown("<h1 style='text-align: center; font-size: 24px;'>åŠŸèƒ½å¯¼èˆª</h1>", unsafe_allow_html=True)
    if 'page' not in st.session_state: st.session_state.page = "çŸ¥è¯†åº“é—®ç­”"
    def set_page(page_name): st.session_state.page = page_name
    st.button("çŸ¥è¯†åº“é—®ç­”", on_click=set_page, args=("çŸ¥è¯†åº“é—®ç­”",), use_container_width=True)
    st.button("æ–‡çŒ®æ£€ç´¢", on_click=set_page, args=("æ–‡çŒ®æ£€ç´¢",), use_container_width=True)
    st.button("XRDåˆ†æ", on_click=set_page, args=("XRDåˆ†æ",), use_container_width=True)
    st.button("æ€§èƒ½é¢„æµ‹", on_click=set_page, args=("æ€§èƒ½é¢„æµ‹",), use_container_width=True)
    st.button("å®éªŒä¼˜åŒ–", on_click=set_page, args=("å®éªŒä¼˜åŒ–",), use_container_width=True)

# --- é¡µé¢æ¸²æŸ“ ---
if st.session_state.page == "çŸ¥è¯†åº“é—®ç­”":
    st.header("ğŸ’¬ æ™ºèƒ½çŸ¥è¯†åº“é—®ç­” (RAG + Qwen)")
    if not os.getenv("DASHSCOPE_API_KEY"):
        st.error("é”™è¯¯ï¼šè¯·å…ˆè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ã€‚")
        st.code("export DASHSCOPE_API_KEY='æ‚¨çš„key'", language="shell")
    else:
        if "messages" not in st.session_state: st.session_state.messages = []
        for message in st.session_state.messages: 
            with st.chat_message(message["role"]): st.markdown(message["content"])
        
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.markdown(prompt)
            
            with st.chat_message("assistant"):
                with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                    retriever = get_retriever()
                    if not retriever: st.error("çŸ¥è¯†åº“ç´¢å¼•æœªæ‰¾åˆ°ï¼")
                    else:
                        relevant_docs = retriever.get_relevant_documents(prompt)
                        use_rag = False
                        if relevant_docs:
                            context_string = "\n\n".join(doc.page_content for doc in relevant_docs)
                            validate_messages = [
                                {"role": "system", "content": "You are a helpful assistant."},
                                {"role": "user", "content": f"ä»…æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š\n\n{context_string}\n\nåˆ¤æ–­æ˜¯å¦å¯ä»¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š'{prompt}'ï¼Ÿè¯·åªå›ç­”'æ˜¯'æˆ–'å¦'ã€‚"}
                            ]
                            validation_result = call_qwen_model(validate_messages)
                            if "æ˜¯" in validation_result:
                                use_rag = True
                        
                        if use_rag:
                            st.info("AIåˆ¤æ–­ä¿¡æ¯ç›¸å…³ï¼Œå°†åŸºäºçŸ¥è¯†åº“å›ç­”...")
                            system_content = f"è¯·ä»…æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ï¼Œå›ç­”æ—¶å¯ä»¥å¯¹ä¿¡æ¯è¿›è¡Œæ€»ç»“å’Œç»„ç»‡ï¼Œä½†ä¸è¦è¶…å‡ºä¸Šä¸‹æ–‡èŒƒå›´:\n{context_string}"
                            messages = [
                                {"role": "system", "content": system_content},
                                {"role": "user", "content": prompt}
                            ]
                            response = call_qwen_model(messages)
                        else:
                            st.warning("AIåˆ¤æ–­çŸ¥è¯†åº“ä¸­æ— ç›´æ¥ç›¸å…³ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é€šç”¨çŸ¥è¯†å›ç­”...")
                            messages = [
                                {"role": "system", "content": "You are a helpful assistant."},
                                {"role": "user", "content": prompt}
                            ]
                            response = call_qwen_model(messages)
                        
                        st.markdown(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})

elif st.session_state.page == "æ–‡çŒ®æ£€ç´¢":
    st.header("ğŸ“° æ–‡çŒ®æ£€ç´¢")
    
    keywords_input = st.text_input(
        "è¯·è¾“å…¥å…³é”®è¯ï¼ˆå¤šä¸ªè¯·ç”¨è‹±æ–‡é€—å·,éš”å¼€ï¼‰:", 
        value="perovskite stability, CsPbI3"
    )
    
    if st.button("å¼€å§‹æ£€ç´¢"):
        if keywords_input:
            keywords_list = [keyword.strip() for keyword in keywords_input.split(',')]
            with st.spinner(f"æ­£åœ¨ä»arXivæ£€ç´¢: {', '.join(keywords_list)}..."):
                report = get_latest_briefing(keywords_list)
                st.markdown(report)
        else:
            st.warning("è¯·è¾“å…¥å…³é”®è¯ã€‚")

elif st.session_state.page == "XRDåˆ†æ":
    st.header("ğŸ“ˆ XRDæ•°æ®è‡ªåŠ¨åˆ†æ")
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ æ‚¨çš„XRDæ•°æ®æ–‡ä»¶", type=["txt", "csv"])
    if uploaded_file:
        st.pyplot(analyze_xrd_from_upload(uploaded_file))

elif st.session_state.page == "æ€§èƒ½é¢„æµ‹":
    st.header("ğŸ’¡ ææ–™æ€§èƒ½é¢„æµ‹")
    model = get_trained_model()
    if model:
        col1, col2, col3 = st.columns(3)
        rpm = col1.slider("æ—‹æ¶‚é€Ÿåº¦ (rpm)", 2000, 6000, 4000, 100)
        temp = col2.slider("é€€ç«æ¸©åº¦ (Â°C)", 80, 140, 100, 5)
        conc = col3.slider("æ·»åŠ å‰‚æµ“åº¦ (%)", 0.1, 2.0, 1.0, 0.1)
        if st.button("é¢„æµ‹æ•ˆç‡"):
            prediction = model.predict(pd.DataFrame({'spin_coating_rpm': [rpm], 'annealing_temperature_C': [temp], 'additive_concentration_percent': [conc]}))
            st.success(f"**é¢„æµ‹çš„å…‰ç”µè½¬æ¢æ•ˆç‡ä¸º: {prediction[0]:.2f} %**")
    else:
        st.error("æ•°æ®æ–‡ä»¶ 'simulated_experimental_data.csv' ä¸å­˜åœ¨ã€‚")

elif st.session_state.page == "å®éªŒä¼˜åŒ–":
    st.header("ğŸš€ å®éªŒæ–¹æ¡ˆä¼˜åŒ–")
    model = get_trained_model()
    if model:
        if st.button("å¼€å§‹ä¼˜åŒ–ï¼Œå¯»æ‰¾æœ€ä½³å‚æ•°"):
            with st.spinner("æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢ä¼˜åŒ–..."):
                params, eff = find_optimal_params(model)
                st.success("ä¼˜åŒ–å®Œæˆï¼AIæ¨èçš„æœ€ä½³å®éªŒå‚æ•°ç»„åˆä¸º:")
                st.table(params)
                st.info(f"**å¯¹åº”çš„æœ€é«˜é¢„æµ‹æ•ˆç‡ä¸º: {eff:.2f} %**")
    else:
        st.error("æ•°æ®æ–‡ä»¶ 'simulated_experimental_data.csv' ä¸å­˜åœ¨ã€‚")
