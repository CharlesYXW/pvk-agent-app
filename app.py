import streamlit as st
import pandas as pd
from dotenv import load_dotenv

# åœ¨æ‰€æœ‰ä»£ç æ‰§è¡Œå‰ï¼Œé¦–å…ˆåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from sklearn.ensemble import RandomForestRegressor
import itertools
import os
import arxiv
import datetime
import base64 # Added for base64 encoding of logo

# LangChainç›¸å…³çš„åº“ï¼ˆä»…ç”¨äºæ£€ç´¢ï¼‰
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings

# DashScopeå®˜æ–¹SDK
from dashscope import Generation

# --- AI Persona Definition ---
JA_ASSISTANT_PERSONA = "ä½ æ˜¯æ™¶æ¾³ç§‘æŠ€ï¼ˆJA SOLARï¼‰é’™é’›çŸ¿ç ”ç©¶éƒ¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåä¸ºâ€˜æ™¶æ¾³æ™ºèƒ½åŠ©æ‰‹â€™ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æä¾›å…‰ä¼è¡Œä¸šç›¸å…³çš„ä¸“ä¸šæ”¯æŒã€‚åœ¨æ‰€æœ‰å›ç­”ä¸­è¯·ä¿æŒè¿™ä¸ªèº«ä»½å’Œä¸“ä¸šçš„è¯­æ°”ã€‚"
JA_ASSISTANT_INTRO = "æ‚¨å¥½ï¼æˆ‘æ˜¯æ™¶æ¾³æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºä¸ºé’™é’›çŸ¿å…‰ä¼ç ”ç©¶æä¾›æ”¯æŒã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•å…³äºé’™é’›çŸ¿æŠ€æœ¯ã€æ–‡çŒ®ã€å®éªŒæ•°æ®åˆ†æç­‰æ–¹é¢çš„é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘æé—®ï¼"

# New persona for general knowledge with strict anti-hallucination rules
JA_ASSISTANT_GENERAL_KNOWLEDGE_PERSONA = """ä½ æ˜¯æ™¶æ¾³ç§‘æŠ€ï¼ˆJA SOLARï¼‰é’™é’›çŸ¿ç ”ç©¶éƒ¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåä¸ºâ€œæ™¶æ¾³æ™ºèƒ½åŠ©æ‰‹â€ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·æä¾›å…‰ä¼è¡Œä¸šç›¸å…³çš„ä¸“ä¸šæ”¯æŒã€‚
åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. ä½ å¯ä»¥ç»“åˆä½ çš„é€šç”¨çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚
2. **æå…¶é‡è¦**: åœ¨ä½¿ç”¨é€šç”¨çŸ¥è¯†å›ç­”æ—¶ï¼Œä½ å¿…é¡»æ˜ç¡®è¯´æ˜è¿™æ˜¯ä¸€ä¸ªè¡Œä¸šå†…çš„æ™®éçŸ¥è¯†æˆ–å…¬å¼€ä¿¡æ¯ï¼Œ**ç»å¯¹ä¸èƒ½**å°†è¿™äº›é€šç”¨çš„æŠ€æœ¯ã€æˆæœæˆ–æ•°æ®å½’åŠŸäºâ€œæ™¶æ¾³ç§‘æŠ€â€ã€‚ä¸èƒ½æé€ ä»»ä½•å…³äºæ™¶æ¾³ç§‘æŠ€çš„ä¿¡æ¯ã€‚
3. åªæœ‰å½“ä½ çš„çŸ¥è¯†åº“ï¼ˆå¦‚æœæä¾›äº†ä¸Šä¸‹æ–‡ï¼‰ä¸­æ˜ç¡®æåˆ°äº†â€œæ™¶æ¾³ç§‘æŠ€â€çš„å…·ä½“æˆæœæ—¶ï¼Œä½ æ‰èƒ½æåŠå…¬å¸ã€‚
4. ä¿æŒä¸“ä¸šã€å®¢è§‚ã€ä¸¥è°¨çš„è¯­æ°”ã€‚"""

# --- æ ¸å¿ƒåŠŸèƒ½é€»è¾‘ ---

# @st.cache_resource
def get_retriever():
    if not os.path.exists("faiss_index"): return None
    try:
        embeddings = SentenceTransformerEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")
        vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        return vectorstore.as_retriever(search_kwargs={"k": 3})
    except Exception as e:
        st.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        return None

# æ–°å¢ï¼šä½¿ç”¨DashScope SDKè°ƒç”¨æ¨¡å‹çš„è¾…åŠ©å‡½æ•°
def call_qwen_model(messages):
    try:
        response = Generation.call(
            model="qwen-flash",
            messages=messages,
            result_format="message",
        )
        if response.status_code == 200:
            return response.output.choices[0].message.content
        else:
            return f"è°ƒç”¨å¤§æ¨¡å‹æ—¶å‡ºé”™ï¼š{response.message}"
    except Exception as e:
        return f"è°ƒç”¨å¤§æ¨¡å‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}"

def summarize_with_ai(summary_text):
    """ä½¿ç”¨Qwenæ¨¡å‹æ€»ç»“è®ºæ–‡æ‘˜è¦ã€‚"""
    prompt = f"è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ä»¥ä¸‹å­¦æœ¯è®ºæ–‡çš„æ‘˜è¦ï¼Œæç‚¼å…¶æ ¸å¿ƒè§‚ç‚¹ã€æ–¹æ³•å’Œç»“è®ºï¼Œä»¥ä¾¿å¿«é€Ÿäº†è§£å…¶ä»·å€¼ã€‚ä¸è¦è¶…è¿‡ä¸‰å¥è¯ã€‚æ‘˜è¦å¦‚ä¸‹ï¼š\n\n{summary_text}"
    messages = [
        {"role": "system", "content": f"{JA_ASSISTANT_PERSONA} åœ¨è¿™ä¸ªå…·ä½“çš„ä»»åŠ¡é‡Œï¼Œä½ çš„è§’è‰²æ˜¯ä¸€ä¸ªä¸“é—¨æ€»ç»“å­¦æœ¯è®ºæ–‡çš„ä¸“å®¶ã€‚"},
        {"role": "user", "content": prompt}
    ]
    return call_qwen_model(messages)

@st.cache_data
def get_latest_papers(keywords, date_range="all_time", sort_by="Relevance"):
    """æ ¹æ®å…³é”®è¯ã€æ—¥æœŸèŒƒå›´å’Œæ’åºæ–¹å¼ä»arXivæ£€ç´¢è®ºæ–‡ã€‚"""
    if not keywords or not any(keywords):
        return [], "è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…³é”®è¯ã€‚"

    # 1. æ„å»ºæ—¥æœŸæŸ¥è¯¢å­—ç¬¦ä¸²
    date_query_part = ""
    if date_range != "all_time":
        end_date = datetime.datetime.now(datetime.timezone.utc)
        if date_range == "last_month":
            start_date = end_date - datetime.timedelta(days=30)
        elif date_range == "last_3_months":
            start_date = end_date - datetime.timedelta(days=90)
        elif date_range == "last_year":
            start_date = end_date - datetime.timedelta(days=365)
        
        start_date_str = start_date.strftime("%Y%m%d%H%M")
        end_date_str = end_date.strftime("%Y%m%d%H%M")
        date_query_part = f" AND submittedDate:[{start_date_str} TO {end_date_str}]"

    # 2. æ£€ç´¢è®ºæ–‡
    # æ ¹æ®æ’åºå‚æ•°é€‰æ‹©APIçš„æ’åºæ ‡å‡†
    api_sort_criterion = arxiv.SortCriterion.Relevance
    if sort_by == "SubmittedDate":
        api_sort_criterion = arxiv.SortCriterion.SubmittedDate

    MAX_RESULTS_PER_KEYWORD = 10
    unique_papers = {}
    for keyword in keywords:
        try:
            full_query = f"({keyword}){date_query_part}"
            search = arxiv.Search(
                query=full_query, 
                max_results=MAX_RESULTS_PER_KEYWORD, 
                sort_by=api_sort_criterion # ä½¿ç”¨é€‰æ‹©çš„æ’åºæ–¹å¼
            )
            for result in search.results():
                if result.entry_id not in unique_papers:
                    unique_papers[result.entry_id] = {
                        "entry_id": result.entry_id,
                        "title": result.title,
                        "authors": ', '.join(author.name for author in result.authors),
                        "pdf_url": result.pdf_url,
                        "summary": result.summary.replace('\n', ' '),
                        "published": result.published.strftime('%Y-%m-%d')
                    }
        except Exception as e:
            return [], f"æ£€ç´¢æ—¶å‡ºé”™: {e}"
    
    if not unique_papers:
        return [], "åœ¨é€‰å®šæ—¶é—´èŒƒå›´å†…ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨å…³é”®è¯ç›¸å…³çš„æ–°è®ºæ–‡ã€‚"
        
    # 3. å¯¹æœ€ç»ˆç»“æœåˆ—è¡¨è¿›è¡Œæ’åº
    papers_list = list(unique_papers.values())
    if sort_by == "SubmittedDate":
        # å¦‚æœç”¨æˆ·é€‰æ‹©æŒ‰æœ€æ–°å‘è¡¨æ’åºï¼Œåˆ™å¯¹åˆå¹¶åçš„åˆ—è¡¨è¿›è¡Œæ’åº
        sorted_papers = sorted(papers_list, key=lambda p: p['published'], reverse=True)
    else:
        # å¦‚æœæŒ‰ç›¸å…³åº¦ï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ··åˆåçš„åˆ—è¡¨ï¼ˆé¡ºåºéƒ¨åˆ†å–å†³äºAPIå’Œåˆå¹¶è¿‡ç¨‹ï¼‰
        sorted_papers = papers_list
    
    return sorted_papers, None

def analyze_xrd_from_upload(uploaded_file):
    # ... (æ­¤å‡½æ•°ä¸å˜)
    if uploaded_file is None: return None
    try:
        data = np.loadtxt(uploaded_file, comments="#", delimiter=",")
        angle, intensity = data[:, 0], data[:, 1]
    except Exception:
        st.error("æ–‡ä»¶è§£æå¤±è´¥ã€‚è¯·ç¡®ä¿æ˜¯ä¸¤åˆ—ï¼ˆè§’åº¦, å¼ºåº¦ï¼‰çš„CSVæˆ–TXTæ–‡ä»¶ã€‚")
        return None
    peaks, _ = find_peaks(intensity, height=np.mean(intensity), distance=10)
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(angle, intensity, label="XRD Spectrum"), ax.plot(angle[peaks], intensity[peaks], "x", markersize=8, label="Detected Peaks")
    for i in peaks: ax.annotate(f"{angle[i]:.2f}Â°", (angle[i], intensity[i]), textcoords="offset points", xytext=(0,5), ha='center')
    ax.set_title("XRD Spectrum Analysis"), ax.set_xlabel("2-Theta Angle (Â°)"), ax.set_ylabel("Intensity (A.U.)")
    ax.legend(), ax.grid(True, linestyle='--', alpha=0.6)
    return fig

@st.cache_resource
def get_trained_model():
    # ... (æ­¤å‡½æ•°ä¸å˜)
    if not os.path.exists("simulated_experimental_data.csv"): return None
    df = pd.read_csv("simulated_experimental_data.csv")
    features = ['spin_coating_rpm', 'annealing_temperature_C', 'additive_concentration_percent']
    target = 'efficiency_percent'
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(df[features], df[target])
    return model

def find_optimal_params(_model):
    # ... (æ­¤å‡½æ•°ä¸å˜)
    features = ['spin_coating_rpm', 'annealing_temperature_C', 'additive_concentration_percent']
    param_grid = list(itertools.product(np.arange(2500, 5501, 500), np.arange(80, 121, 10), np.arange(0.5, 1.51, 0.2)))
    grid_df = pd.DataFrame(param_grid, columns=features)
    predicted_efficiencies = _model.predict(grid_df)
    best_index = np.argmax(predicted_efficiencies)
    return grid_df.iloc[best_index], predicted_efficiencies[best_index]

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(
    page_title="æ™¶æ¾³ç ”å‘æ™ºèƒ½åŠ©æ‰‹",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded",
)
st.markdown(
    """
    <style>
       .block-container {
            padding-top: 1.5rem !important;
        }
        /* Custom styling for the 'AI Summary' button using a more robust data-testid selector */
        div[data-testid="stHorizontalBlock"] > div:nth-child(2) .stButton button {
            background-color: #4A90E2 !important; /* A medium-dark blue */
            border-color: #4A90E2 !important;
            color: white !important; /* White text for readability */
        }
        div[data-testid="stHorizontalBlock"] > div:nth-child(2) .stButton button:hover {
            background-color: #357ABD !important; /* A slightly darker blue for hover */
            border-color: #357ABD !important;
            color: white !important;
        }
    </style>
    """,
    unsafe_allow_html=True)
st.title("ğŸ”¬ é’™é’›çŸ¿ç ”å‘æ™ºèƒ½åŠ©æ‰‹")

# --- å¯¼èˆª ---
with st.sidebar:
    script_dir = os.path.dirname(os.path.abspath(__file__))
    logo_path = os.path.join(script_dir, "assets", "logo.png")
    st.image(logo_path, use_container_width=True)

    st.markdown("<h1 style='text-align: center; font-size: 24px;'>åŠŸèƒ½å¯¼èˆª</h1>", unsafe_allow_html=True)
    if 'page' not in st.session_state: st.session_state.page = "çŸ¥è¯†åº“é—®ç­”"
    def set_page(page_name): st.session_state.page = page_name
    st.button("çŸ¥è¯†åº“é—®ç­”", on_click=set_page, args=("çŸ¥è¯†åº“é—®ç­”",), use_container_width=True)
    st.button("æ–‡çŒ®æ£€ç´¢", on_click=set_page, args=("æ–‡çŒ®æ£€ç´¢",), use_container_width=True)
    st.button("XRDåˆ†æ", on_click=set_page, args=("XRDåˆ†æ",), use_container_width=True)
    st.button("æ€§èƒ½é¢„æµ‹", on_click=set_page, args=("æ€§èƒ½é¢„æµ‹",), use_container_width=True)
    st.button("å®éªŒä¼˜åŒ–", on_click=set_page, args=("å®éªŒä¼˜åŒ–",), use_container_width=True)

# --- é¡µé¢æ¸²æŸ“ ---
if st.session_state.page == "çŸ¥è¯†åº“é—®ç­”":
    st.header("ğŸ’¬ æ™ºèƒ½çŸ¥è¯†åº“é—®ç­” (RAG + Qwen)")
    st.markdown("åŸºäºå†…éƒ¨çŸ¥è¯†æ–‡æ¡£ï¼Œæä¾›ç²¾å‡†çš„é—®ç­”èƒ½åŠ›ã€‚å¦‚æœæ–‡æ¡£æ— ç›¸å…³ä¿¡æ¯ï¼Œå°†ç”±å¤§æ¨¡å‹æä¾›é€šç”¨å›ç­”ã€‚")

    with st.container(border=True):
        if not os.getenv("DASHSCOPE_API_KEY"):
            st.error("é”™è¯¯ï¼šè¯·å…ˆè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ã€‚")
            st.code("export DASHSCOPE_API_KEY='æ‚¨çš„key'", language="shell")
        else:
            # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å’Œå¼€åœºç™½
            if "messages" not in st.session_state:
                st.session_state.messages = [{"role": "assistant", "content": JA_ASSISTANT_INTRO}]

            # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥å±•ç¤ºèŠå¤©è®°å½•
            chat_box = st.container(height=400)

            # åœ¨å®¹å™¨ä¸­æ˜¾ç¤ºå†å²æ¶ˆæ¯
            for message in st.session_state.messages:
                with chat_box.chat_message(message["role"]):
                    st.markdown(message["content"])
            
            # èŠå¤©è¾“å…¥æ¡†
            if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
                # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²å¹¶ç«‹å³æ˜¾ç¤ºåœ¨å®¹å™¨ä¸­
                st.session_state.messages.append({"role": "user", "content": prompt})
                with chat_box.chat_message("user"):
                    st.markdown(prompt)
                
                # è·å–å¹¶æ˜¾ç¤ºåŠ©æ‰‹çš„å›å¤
                with chat_box.chat_message("assistant"):
                    with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                        retriever = get_retriever()
                        if not retriever:
                            st.error("çŸ¥è¯†åº“ç´¢å¼•æœªæ‰¾åˆ°ï¼è¯·å…ˆè¿è¡Œ `build_knowledge_base.py` æ¥åˆ›å»ºçŸ¥è¯†åº“ã€‚")
                            st.stop()

                        # RAGé€»è¾‘...
                        relevant_docs = retriever.get_relevant_documents(prompt)
                        use_rag = False
                        if relevant_docs:
                            context_string = "\n\n".join(doc.page_content for doc in relevant_docs)
                            validate_messages = [
                                {"role": "system", "content": "You are a helpful assistant."},
                                {"role": "user", "content": f"ä»…æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š\n\n{context_string}\n\nåˆ¤æ–­æ˜¯å¦å¯ä»¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š'{prompt}'ï¼Ÿè¯·åªå›ç­”'æ˜¯'æˆ–'å¦'ã€‚"}
                            ]
                            validation_result = call_qwen_model(validate_messages)
                            if "æ˜¯" in validation_result:
                                use_rag = True
                        
                        if use_rag:
                            st.info("âœ… AIåˆ¤æ–­ä¿¡æ¯ç›¸å…³ï¼Œå°†åŸºäºçŸ¥è¯†åº“å›ç­”...")
                            system_content = f"{JA_ASSISTANT_PERSONA} è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ï¼Œå›ç­”æ—¶å¯ä»¥å¯¹ä¿¡æ¯è¿›è¡Œæ€»ç»“å’Œç»„ç»‡ï¼Œä½†ä¸è¦è¶…å‡ºä¸Šä¸‹æ–‡èŒƒå›´:\n{context_string}"
                            messages = [{"role": "system", "content": system_content}, {"role": "user", "content": prompt}]
                            response = call_qwen_model(messages)
                        else:
                            st.warning("âš ï¸ AIåˆ¤æ–­çŸ¥è¯†åº“ä¸­æ— ç›´æ¥ç›¸å…³ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é€šç”¨çŸ¥è¯†å›ç­”...")
                            messages = [{"role": "system", "content": JA_ASSISTANT_GENERAL_KNOWLEDGE_PERSONA}, {"role": "user", "content": prompt}]
                            response = call_qwen_model(messages)
                        
                        st.markdown(response)
                        # å°†åŠ©æ‰‹å›å¤ä¹Ÿæ·»åŠ åˆ°ä¼šè¯çŠ¶æ€
                        st.session_state.messages.append({"role": "assistant", "content": response})

elif st.session_state.page == "æ–‡çŒ®æ£€ç´¢":
    st.header("ğŸ“° æœ€æ–°ç§‘ç ”æ–‡çŒ®è¿½è¸ª")
    st.markdown("è¾“å…¥å…³é”®è¯ï¼ŒAIå°†è‡ªåŠ¨ä»arXivä¸Šæ£€ç´¢æœ€æ–°çš„ç›¸å…³è®ºæ–‡ï¼Œå¹¶ç”Ÿæˆç®€æŠ¥ã€‚")

    # åˆå§‹åŒ–AIæ‘˜è¦çš„çŠ¶æ€å­˜å‚¨
    if 'ai_summaries' not in st.session_state:
        st.session_state.ai_summaries = {}
    if 'search_results' not in st.session_state:
        st.session_state.search_results = None

    with st.container(border=True):
        # ä½¿ç”¨åˆ—æ¥å¸ƒå±€è¾“å…¥æ¡†å’Œé€‰æ‹©å™¨
        col1, col2, col3 = st.columns([3, 1, 1])
        with col1:
            keywords_input = st.text_input(
                "è¯·è¾“å…¥å…³é”®è¯ï¼ˆå¤šä¸ªè¯·ç”¨è‹±æ–‡é€—å·,éš”å¼€ï¼‰:", 
                value="perovskite stability, CsPbI3",
                help="ä¾‹å¦‚: perovskite solar cell, ETL, device stability"
            )
        with col2:
            date_range = st.selectbox(
                "æ—¶é—´èŒƒå›´",
                ("all_time", "last_month", "last_3_months", "last_year"),
                format_func=lambda x: {
                    "all_time": "æ‰€æœ‰æ—¶é—´",
                    "last_month": "æœ€è¿‘ä¸€ä¸ªæœˆ",
                    "last_3_months": "æœ€è¿‘ä¸‰ä¸ªæœˆ",
                    "last_year": "æœ€è¿‘ä¸€å¹´"
                }.get(x),
            )
        with col3:
            sort_by = st.selectbox(
                "æ’åºæ–¹å¼",
                ("Relevance", "SubmittedDate"),
                index=1, # é»˜è®¤é€‰æ‹©â€œæœ€æ–°å‘è¡¨â€
                format_func=lambda x: {"Relevance": "ç›¸å…³åº¦", "SubmittedDate": "æœ€æ–°å‘è¡¨"}.get(x)
            )

        if st.button("å¼€å§‹æ£€ç´¢", use_container_width=True):
            # æ¸…ç©ºä¹‹å‰çš„AIæ‘˜è¦å’Œç»“æœ
            st.session_state.ai_summaries = {}
            st.session_state.search_results = None
            st.session_state.search_error = None
            if keywords_input:
                keywords_list = [keyword.strip() for keyword in keywords_input.split(',') if keyword.strip()]
                with st.spinner(f"æ­£åœ¨ä»arXivæ£€ç´¢: {', '.join(keywords_list)}...\nè¯·è€å¿ƒç­‰å¾…ï¼Œæ£€ç´¢å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚"):
                    papers, error = get_latest_papers(keywords_list, date_range, sort_by)
                    # å°†ç»“æœå­˜å‚¨åœ¨session stateä¸­ï¼Œä»¥ä¾¿åœ¨æŒ‰é’®ç‚¹å‡»åä¿ç•™
                    st.session_state.search_results = papers
                    st.session_state.search_error = error
            else:
                st.warning("è¯·è¾“å…¥å…³é”®è¯ã€‚")

    # åœ¨ä¸»æŒ‰é’®é€»è¾‘å¤–éƒ¨æ¸²æŸ“ç»“æœï¼Œä»¥æ”¯æŒAIæ€»ç»“æŒ‰é’®çš„äº¤äº’
    if st.session_state.search_results:
        papers = st.session_state.search_results
        st.success(f"æ£€ç´¢å®Œæˆï¼å…±æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡ã€‚")
        
        for i, paper in enumerate(papers):
            with st.expander(f"**{i+1}. {paper['title']}**", expanded=True):
                st.markdown(f"**å‘è¡¨æ—¥æœŸ:** {paper['published']} | **ä½œè€…:** {paper['authors']}")
                st.markdown(f"**æ‘˜è¦:** {paper['summary']}")
                
                # åŠŸèƒ½æŒ‰é’® - ä¼˜åŒ–å¸ƒå±€
                col1, col2, col3 = st.columns(3, gap="small")
                with col1:
                    st.link_button("é˜…è¯»åŸæ–‡", paper['pdf_url'], use_container_width=True)
                with col2:
                    if st.button("AIæ€»ç»“", key=f"summarize_{paper['entry_id']}", use_container_width=True):
                        with st.spinner("AIæ­£åœ¨é˜…è¯»æ‘˜è¦ï¼Œè¯·ç¨å€™..."):
                            ai_summary = summarize_with_ai(paper['summary'])
                            st.session_state.ai_summaries[paper['entry_id']] = ai_summary
                with col3:
                    if st.button("æ·±å…¥ç ”ç©¶", key=f"research_{paper['entry_id']}", use_container_width=True):
                        st.toast("è¯¥åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")

                # å¦‚æœå­˜åœ¨AIæ€»ç»“ï¼Œåˆ™æ˜¾ç¤ºå®ƒ
                if paper['entry_id'] in st.session_state.ai_summaries:
                    st.info(f"{st.session_state.ai_summaries[paper['entry_id']]}")

    elif st.session_state.get('search_error'):
        st.error(st.session_state.search_error)
    # åªæœ‰åœ¨æŒ‰é’®è¢«ç‚¹å‡»åï¼Œsearch_resultsæ‰ä¼šè¢«å®šä¹‰ï¼Œæ‰€ä»¥éœ€è¦æ£€æŸ¥
    elif st.session_state.get('search_results') is not None and not st.session_state.get('search_results'):
        st.warning("åœ¨é€‰å®šæ—¶é—´èŒƒå›´å†…ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨å…³é”®è¯ç›¸å…³çš„æ–°è®ºæ–‡ã€‚")

elif st.session_state.page == "XRDåˆ†æ":
    st.header("ğŸ“ˆ XRDæ•°æ®è‡ªåŠ¨åˆ†æ")
    st.markdown("ä¸Šä¼ æ‚¨çš„åŸå§‹XRDæ•°æ®æ–‡ä»¶ï¼ˆtxtæˆ–csvæ ¼å¼ï¼‰ï¼ŒAIå°†è‡ªåŠ¨ç»˜åˆ¶å›¾è°±å¹¶è¯†åˆ«ä¸»è¦è¡å°„å³°ã€‚")

    with st.container(border=True):
        uploaded_file = st.file_uploader(
            "è¯·åœ¨æ­¤å¤„ä¸Šä¼ æ‚¨çš„XRDæ•°æ®æ–‡ä»¶", 
            type=["txt", "csv"],
            label_visibility="collapsed"
        )
        if uploaded_file:
            with st.spinner("æ­£åœ¨åˆ†æå›¾è°±..."):
                fig = analyze_xrd_from_upload(uploaded_file)
                if fig:
                    st.pyplot(fig)
                    st.success("å›¾è°±ç”Ÿæˆå®Œæ¯•ï¼")

elif st.session_state.page == "æ€§èƒ½é¢„æµ‹":
    st.header("ğŸ’¡ ææ–™æ€§èƒ½é¢„æµ‹")
    st.markdown("è°ƒæ•´ä»¥ä¸‹å®éªŒå‚æ•°ï¼ŒAIæ¨¡å‹å°†é¢„æµ‹å¯¹åº”çš„å…‰ç”µè½¬æ¢æ•ˆç‡ã€‚")
    
    model = get_trained_model()
    if model:
        # ä½¿ç”¨å¸¦è¾¹æ¡†çš„å®¹å™¨æ¥ç»„ç»‡UI
        with st.container(border=True):
            col1, col2, col3 = st.columns(3)
            with col1:
                rpm = st.slider("æ—‹æ¶‚é€Ÿåº¦ (rpm)", 2000, 6000, 4000, 100)
            with col2:
                temp = st.slider("é€€ç«æ¸©åº¦ (Â°C)", 80, 140, 100, 5)
            with col3:
                conc = st.slider("æ·»åŠ å‰‚æµ“åº¦ (%)", 0.1, 2.0, 1.0, 0.1)
        
        # å°†æŒ‰é’®å’Œç»“æœä¹Ÿæ”¾åœ¨ä¸€ä¸ªå®¹å™¨ä¸­
        with st.container(border=True):
            if st.button("æ‰§è¡Œé¢„æµ‹", use_container_width=True):
                new_params = pd.DataFrame({
                    'spin_coating_rpm': [rpm],
                    'annealing_temperature_C': [temp],
                    'additive_concentration_percent': [conc]
                })
                prediction = model.predict(new_params)
                st.metric(label="é¢„æµ‹æ•ˆç‡", value=f"{prediction[0]:.2f} %")
    else:
        st.error("æ•°æ®æ–‡ä»¶ 'simulated_experimental_data.csv' ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ã€‚")

elif st.session_state.page == "å®éªŒä¼˜åŒ–":
    st.header("ğŸš€ å®éªŒæ–¹æ¡ˆä¼˜åŒ–")
    st.markdown("AIå°†æœç´¢å¤šç§å‚æ•°ç»„åˆï¼Œä¸ºæ‚¨æ¨èèƒ½äº§ç”Ÿæœ€é«˜æ•ˆç‡çš„â€˜æœ€ä½³å®éªŒæ–¹æ¡ˆâ€™ã€‚")
    
    model = get_trained_model()
    if model:
        with st.container(border=True):
            st.info("è¯·æ³¨æ„ï¼šä¸ºä¿è¯å¿«é€Ÿå“åº”ï¼Œæ¼”ç¤ºç‰ˆçš„æœç´¢ç©ºé—´è¾ƒå°ã€‚åœ¨è·å–æ›´å¤šçœŸå®æ•°æ®åï¼Œå¯æ‰©å±•æœç´¢èŒƒå›´ä»¥è·å¾—æ›´ä¼˜ç»“æœã€‚")
            if st.button("å¼€å§‹ä¼˜åŒ–ï¼Œå¯»æ‰¾æœ€ä½³å‚æ•°", use_container_width=True):
                with st.spinner("æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢ä¼˜åŒ–..."):
                    params, eff = find_optimal_params(model)
                    st.success("ä¼˜åŒ–å®Œæˆï¼")
                    st.metric(label="æœ€é«˜é¢„æµ‹æ•ˆç‡", value=f"{eff:.2f} %")
                    
                    st.write("AIæ¨èçš„æœ€ä½³å®éªŒå‚æ•°ç»„åˆä¸º:")
                    st.table(params)
    else:
        st.error("æ•°æ®æ–‡ä»¶ 'simulated_experimental_data.csv' ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œä¼˜åŒ–ã€‚")
